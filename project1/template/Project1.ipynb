{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data(DATA_FOLDER + 'train.csv')\n",
    "y_test, x_test, id_test = load_csv_data(DATA_FOLDER + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.filter(lambda v: v==v, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100000, 100001, 100002, ..., 349997, 349998, 349999])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_prediction(w_train, x, y):\n",
    "    y_pred = predict_labels(w_train, x)\n",
    "    return (y_pred == y).sum()/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "np.shape(w_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74432799999999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_ls, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.asarray([1]*30)[:,None].reshape(30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.asarray([1]*30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.asarray([1]*30)\n",
    "max_iters = 30\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkappel/Desktop/EPFL/Master1/ML/Isaac/ML_project/project1/template/loss.py:5: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean(e**2)/2.\n"
     ]
    }
   ],
   "source": [
    "w_sgd, loss_sgd = least_squares_SGD(y_train, x_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62795599999999996"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_sgd, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_polynomial import build_poly\n",
    "\n",
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # split data\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    tr_poly = build_poly(x_tr, degree)\n",
    "    te_poly = build_poly(x_te, degree)\n",
    "    print(np.shape(x_tr))\n",
    "    print(np.shape(tr_poly))\n",
    "    #####\n",
    "    #tr_poly = np.sum(np.split(tr_poly, degree+1, axis = 1), axis=0)\n",
    "    #te_poly = np.sum(np.split(te_poly, degree+1, axis = 1), axis=0)\n",
    "    #####\n",
    "    \n",
    "    loss = 1000\n",
    "    w = 0\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        # ridge regression\n",
    "        weight, l = ridge_regression(y_tr, tr_poly, lambda_)\n",
    "        #weight = stack_w(weight, degree)\n",
    "        mse_test = compute_loss(y_te, te_poly, weight)\n",
    "        \n",
    "        if(mse_test < loss):\n",
    "            loss = mse_test\n",
    "            w = weight\n",
    "        \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225000, 30)\n",
      "(225000, 210)\n"
     ]
    }
   ],
   "source": [
    "# boucler sur les seeds pour avoir un meileur ? \n",
    "seed = 69\n",
    "degree = 6\n",
    "split_ratio = 0.9\n",
    "\n",
    "w_r, loss_ridge = ridge_regression_demo(x_train, y_train, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(w_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_s = np.sum(np.split(w_r, degree + 1),axis=0)\n",
    "np.shape(w_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 210)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(build_poly(x_train, degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79982799999999998"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_r.reshape(len(w_r), 1), build_poly(x_train, degree), y_train.reshape(len(y_train), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression2(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = lamb * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def ridge_regression3(y, tx, lambda_):\n",
    "    first_term = tx.T.dot(tx)\n",
    "    left = first_term + lambda_ *np.identity(tx.shape[1])\n",
    "    right = tx.T.dot(y)\n",
    "    w = np.linalg.solve(left, right)\n",
    "    loss = compute_loss2(y, tx, w)\n",
    "    return w, loss;\n",
    "\n",
    "def compute_loss2(y, tx, w):\n",
    "    e = y - tx.dot(w)\n",
    "    return np.mean(e**2)/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from implementations import least_squares\n",
    "from split_data import split_data\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    num_data = 10000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    \n",
    "    best_seed = 0\n",
    "    best_degree = 0\n",
    "    best_lambda = 0\n",
    "    min_loss = 1000\n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "        # split data with a specific seed\n",
    "        x_tr, y_tr, x_te, y_te = split_data(x_train, y_train, ratio_train, seed)\n",
    "        \n",
    "        # bias_variance_decomposition\n",
    "        for index_deg, degree in enumerate(degrees):\n",
    "            # form data with polynomial degree\n",
    "            train_poly = build_poly(x_tr, degree)\n",
    "            test_poly = build_poly(x_te, degree)\n",
    "            \n",
    "            w_train, loss_train = least_squares(y_tr, train_poly)\n",
    "\n",
    "            # calculate the loss for train and test data\n",
    "            loss_test = compute_loss(y_te, test_poly, w_train)\n",
    "            \n",
    "            if(loss_test < min_loss):\n",
    "                min_loss = loss_test\n",
    "                best_seed = seed\n",
    "                #best_degree = degree\n",
    "                best_lambda = lambda_\n",
    "    #return best_seed, best_lambda, min_loss\n",
    "    return best_seed, best_degree ,min_loss        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s, d, l = bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_k_indices import build_k_indices\n",
    "from implementations import ridge_regression\n",
    "from build_polynomial import build_poly\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    train_indices = [x for j, x in enumerate(k_indices) if j != k]\n",
    "    train_indices = [idx for part in train_indices for idx in part]\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_tr = x[train_indices]\n",
    "    y_tr = y[train_indices]\n",
    "    \n",
    "    x_te = x[test_indices]\n",
    "    y_te = y[test_indices]\n",
    "\n",
    "    # form data with polynomial degree\n",
    "    #train_poly = build_poly(x_tr, degree)\n",
    "    #test_poly = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression\n",
    "    \n",
    "    weights_train, loss_tr = ridge_regression(y_tr, x_tr, lambda_)\n",
    "\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_te, x_te, weights_train)\n",
    "\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 55\n",
    "    degree = 2\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    \n",
    "    min_mean_loss = 1000\n",
    "    best_lambdas = np.zeros(k_fold)\n",
    "    # cross validation\n",
    "    for i_test in range(k_fold):\n",
    "        min_loss = 1000\n",
    "        for lambda_ in lambdas:\n",
    "            loss_tr, loss_te = cross_validation(y_train, x_train, k_indices, i_test, lambda_, degree)\n",
    "            \n",
    "            if(loss_te < min_loss):\n",
    "                min_loss = loss_te\n",
    "                best_lambdas[i_test] = lambda_\n",
    "    lambda_mean = np.mean(best_lambdas)\n",
    "    return lambda_mean, min_mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bl, mml = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25007499999999999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, l = ridge_regression(y_train, x_train, bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74440799999999996"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(250000, 1)\n"
     ]
    }
   ],
   "source": [
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "\n",
    "x_train2 = x_train\n",
    "y_train2 = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "print(np.shape(x_train2))\n",
    "print(np.shape(y_train2))\n",
    "\n",
    "#tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "w = np.zeros((x_train2.shape[1], 1))\n",
    "\n",
    "w , loss = logistic_regression(y_train2, x_train2, w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173286.79514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loss)\n",
    "np.shape(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#marche pas ... a voir\n",
    "\n",
    "#compare_prediction(w, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_demo():\n",
    "    max_iters = 2\n",
    "    #gamma = 0.01\n",
    "    gammas = np.arange(0.01,0.5,0.01)\n",
    "    \n",
    "    x_train2 = x_train\n",
    "    y_train2 = y_train.reshape(len(y_train), 1)\n",
    "    \n",
    "    w = np.zeros((x_train2.shape[1], 1))\n",
    "    \n",
    "    best_w = 0\n",
    "    best_loss = 1000\n",
    "    best_gamma = 0\n",
    "    \n",
    "    for gamma_ in gammas:\n",
    "        \n",
    "        w , loss = logistic_regression(y_train2, x_train2, w, max_iters, gamma_)\n",
    "        \n",
    "        #print(gamma_)\n",
    "        #print(loss)\n",
    "        \n",
    "        if(loss < best_loss ):\n",
    "            best_loss = loss\n",
    "            best_gamma = gamma_\n",
    "            best_w = w\n",
    "\n",
    "    \n",
    "    return best_w, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkappel/Desktop/EPFL/Master1/ML/Isaac/ML_project/project1/template/loss.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  loss = np.log(1 + np.exp(tx @ w)) - (y * (tx @ w))\n",
      "/Users/kevinkappel/Desktop/EPFL/Master1/ML/Isaac/ML_project/project1/template/sigmoid.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(t) / (1 + np.exp(t)))\n",
      "/Users/kevinkappel/Desktop/EPFL/Master1/ML/Isaac/ML_project/project1/template/sigmoid.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (np.exp(t) / (1 + np.exp(t)))\n"
     ]
    }
   ],
   "source": [
    "# Problem\n",
    "w, loss = logistic_regression_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
