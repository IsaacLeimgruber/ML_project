{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#from implementations import *\n",
    "DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data(DATA_FOLDER + 'train.csv')\n",
    "y_test, x_test, id_test = load_csv_data(DATA_FOLDER + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train.filter(lambda v: v==v, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100000, 100001, 100002, ..., 349997, 349998, 349999])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_prediction(w_train, x, y):\n",
    "    y_pred = predict_labels(w_train, x)\n",
    "    return (y_pred == y).sum()/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "np.shape(w_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74432799999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_ls, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.asarray([1]*30)[:,None].reshape(30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.asarray([1]*30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.asarray([1]*30)\n",
    "max_iters = 30\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkappel/Desktop/EPFL/Master1/ML/Isaac/ML_project/project1/template/loss.py:5: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean(e**2)/2.\n"
     ]
    }
   ],
   "source": [
    "w_sgd, loss_sgd = least_squares_SGD(y_train, x_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62978400000000001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_sgd, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_polynomial import build_poly\n",
    "\n",
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # split data\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    tr_poly = build_poly(x_tr, degree)\n",
    "    te_poly = build_poly(x_te, degree)\n",
    "    print(np.shape(x_tr))\n",
    "    print(np.shape(tr_poly))\n",
    "    #####\n",
    "    #tr_poly = np.sum(np.split(tr_poly, degree+1, axis = 1), axis=0)\n",
    "    #te_poly = np.sum(np.split(te_poly, degree+1, axis = 1), axis=0)\n",
    "    #####\n",
    "    \n",
    "    loss = 1000\n",
    "    w = 0\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        # ridge regression\n",
    "        weight, l = ridge_regression(y_tr, tr_poly, lambda_)\n",
    "        #weight = stack_w(weight, degree)\n",
    "        mse_test = compute_loss(y_te, te_poly, weight)\n",
    "        \n",
    "        if(mse_test < loss):\n",
    "            loss = mse_test\n",
    "            w = weight\n",
    "        \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225000, 30)\n",
      "(225000, 210)\n"
     ]
    }
   ],
   "source": [
    "# boucler sur les seeds pour avoir un meileur ? \n",
    "seed = 69\n",
    "degree = 6\n",
    "split_ratio = 0.9\n",
    "\n",
    "w_r, loss_ridge = ridge_regression_demo(x_train, y_train, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(w_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_s = np.sum(np.split(w_r, degree + 1),axis=0)\n",
    "np.shape(w_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 210)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(build_poly(x_train, degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79982799999999998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_r.reshape(len(w_r), 1), build_poly(x_train, degree), y_train.reshape(len(y_train), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression2(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = lamb * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def ridge_regression3(y, tx, lambda_):\n",
    "    first_term = tx.T.dot(tx)\n",
    "    left = first_term + lambda_ *np.identity(tx.shape[1])\n",
    "    right = tx.T.dot(y)\n",
    "    w = np.linalg.solve(left, right)\n",
    "    loss = compute_loss2(y, tx, w)\n",
    "    return w, loss;\n",
    "\n",
    "def compute_loss2(y, tx, w):\n",
    "    e = y - tx.dot(w)\n",
    "    return np.mean(e**2)/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from implementations import least_squares\n",
    "from split_data import split_data\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    num_data = 10000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    \n",
    "    best_seed = 0\n",
    "    best_degree = 0\n",
    "    best_lambda = 0\n",
    "    min_loss = 1000\n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "        # split data with a specific seed\n",
    "        x_tr, y_tr, x_te, y_te = split_data(x_train, y_train, ratio_train, seed)\n",
    "        \n",
    "        # bias_variance_decomposition\n",
    "        for index_deg, degree in enumerate(degrees):\n",
    "            # form data with polynomial degree\n",
    "            train_poly = build_poly(x_tr, degree)\n",
    "            test_poly = build_poly(x_te, degree)\n",
    "            \n",
    "            w_train, loss_train = least_squares(y_tr, train_poly)\n",
    "\n",
    "            # calculate the loss for train and test data\n",
    "            loss_test = compute_loss(y_te, test_poly, w_train)\n",
    "            \n",
    "            if(loss_test < min_loss):\n",
    "                min_loss = loss_test\n",
    "                best_seed = seed\n",
    "                #best_degree = degree\n",
    "                best_lambda = lambda_\n",
    "    #return best_seed, best_lambda, min_loss\n",
    "    return best_seed, best_degree ,min_loss        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#s, d, l = bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_k_indices import build_k_indices\n",
    "from implementations import ridge_regression\n",
    "from build_polynomial import build_poly\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    train_indices = [x for j, x in enumerate(k_indices) if j != k]\n",
    "    train_indices = [idx for part in train_indices for idx in part]\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_tr = x[train_indices]\n",
    "    y_tr = y[train_indices]\n",
    "    \n",
    "    x_te = x[test_indices]\n",
    "    y_te = y[test_indices]\n",
    "\n",
    "    # form data with polynomial degree\n",
    "    #train_poly = build_poly(x_tr, degree)\n",
    "    #test_poly = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression\n",
    "    \n",
    "    weights_train, loss_tr = ridge_regression(y_tr, x_tr, lambda_)\n",
    "\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_te, x_te, weights_train)\n",
    "\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 55\n",
    "    degree = 2\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    \n",
    "    min_mean_loss = 1000\n",
    "    best_lambdas = np.zeros(k_fold)\n",
    "    # cross validation\n",
    "    for i_test in range(k_fold):\n",
    "        min_loss = 1000\n",
    "        for lambda_ in lambdas:\n",
    "            loss_tr, loss_te = cross_validation(y_train, x_train, k_indices, i_test, lambda_, degree)\n",
    "            \n",
    "            if(loss_te < min_loss):\n",
    "                min_loss = loss_te\n",
    "                best_lambdas[i_test] = lambda_\n",
    "    lambda_mean = np.mean(best_lambdas)\n",
    "    return lambda_mean, min_mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bl, mml = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25007499999999999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, l = ridge_regression(y_train, x_train, bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74440799999999996"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30)\n",
      "(2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters = 1\n",
    "gamma = 0.5\n",
    "lambda_ = 0.9\n",
    "\n",
    "x_train2 = x_train[:2]\n",
    "y_train2 = y_train.reshape(len(y_train), 1)\n",
    "y_train2 = y_train2[:2]\n",
    "\n",
    "print(np.shape(x_train2))\n",
    "print(np.shape(y_train2))\n",
    "\n",
    "tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train2]\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "w , loss = logistic_regression(y_train2, tx, initial_w, max_iters, gamma)\n",
    "#w , loss = reg_logistic_regression(y_train2, tx, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38629436112\n",
      "(2, 30)\n",
      "(2, 31)\n",
      "(31, 1)\n",
      "(2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -1.00000000e-02],\n",
       "       [ -1.72170500e+00],\n",
       "       [ -7.73245000e-01],\n",
       "       [ -1.05939000e+00],\n",
       "       [ -5.82290000e-01],\n",
       "       [  1.49895500e+01],\n",
       "       [  1.56085550e+01],\n",
       "       [  1.49983300e+01],\n",
       "       [ -3.67750000e-02],\n",
       "       [  1.78470000e-01],\n",
       "       [ -8.88555000e-01],\n",
       "       [ -5.27500000e-03],\n",
       "       [ -1.42300000e-02],\n",
       "       [  1.49860000e+01],\n",
       "       [ -4.67020000e-01],\n",
       "       [ -2.55000000e-02],\n",
       "       [  4.70700000e-02],\n",
       "       [ -2.95640000e-01],\n",
       "       [  3.85000000e-03],\n",
       "       [ -1.36150000e-02],\n",
       "       [ -5.86440000e-01],\n",
       "       [  2.73550000e-02],\n",
       "       [ -1.17452500e+00],\n",
       "       [ -5.00000000e-03],\n",
       "       [ -3.56215000e-01],\n",
       "       [ -1.25000000e-04],\n",
       "       [ -1.51500000e-02],\n",
       "       [  1.52153100e+01],\n",
       "       [  1.49912000e+01],\n",
       "       [  1.49726250e+01],\n",
       "       [ -1.25905000e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loss)\n",
    "print(np.shape(x_train2))\n",
    "print(np.shape(tx))\n",
    "print(np.shape(w))\n",
    "print(np.shape(y_train2))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marche pas ... a voir\n",
    "\n",
    "#compare_prediction(w, tx, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_demo():\n",
    "    max_iters = 10\n",
    "    #gamma = 0.01\n",
    "    gammas = np.arange(0.01,0.5,0.01)\n",
    "    \n",
    "    x_train2 = x_train\n",
    "    y_train2 = y_train.reshape(len(y_train), 1)\n",
    "    \n",
    "    tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train2]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    best_w = 0\n",
    "    best_loss = 1000\n",
    "    best_gamma = 0\n",
    "    \n",
    "    for gamma_ in gammas:\n",
    "        \n",
    "        w_log , loss = logistic_regression(y_train2, tx, w, max_iters, gamma_)\n",
    "        \n",
    "        #print(gamma_)\n",
    "        print(loss)\n",
    "        \n",
    "        if(loss < best_loss ):\n",
    "            best_loss = loss\n",
    "            best_gamma = gamma_\n",
    "            best_w = w_log\n",
    "\n",
    "    \n",
    "    return best_w, best_loss, best_gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n",
      "173286.79514\n"
     ]
    }
   ],
   "source": [
    "# Problem\n",
    "w, loss, gamma = logistic_regression_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    #return 2*(np.exp(t) / (1 + np.exp(t))) - 1\n",
    "    return (np.exp(t) -1) / (np.exp(t)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    sig = sigmoid(tx.dot(w))\n",
    "    loss = y.T @ (np.log(sig)) + (1 - y).T @ (np.log(1 - sig))\n",
    "    return np.squeeze(- loss)\n",
    "\n",
    "def calculate_loss2(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    \n",
    "    loss = np.log(1 + np.exp(tx @ w)) - (y * (tx @ w))\n",
    "    \n",
    "    sum_loss = np.sum(loss)\n",
    "    \n",
    "    return sum_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    \n",
    "    return  tx.T @(sigmoid(tx @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkappel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/kevinkappel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/kevinkappel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 2\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.9\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        #loss, w = learning_by_newton_method(y, tx, w)\n",
    "        #loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w\n",
    "\n",
    "nb = 20000\n",
    "\n",
    "x_train2 = x_train[:nb]\n",
    "y_train2 = y_train.reshape(len(y_train), 1)\n",
    "y_train2 = y_train2[:nb]\n",
    "\n",
    "loss, w = logistic_regression_gradient_descent_demo(y_train2, x_train2)\n",
    "#logistic_regression_newton_method_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13862.943611198905"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    pred = np.diag(pred.T[0])\n",
    "    r = np.multiply(pred, (1-pred))\n",
    "    return tx.T.dot(r).dot(tx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    hessian = calculate_hessian(y, tx, w)\n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "    w -= np.linalg.solve(hessian, gradient)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    # return loss, gradient, and hessian\n",
    "    loss = calculate_loss(y, tx, w) + (w.T@w*lambda_ / 2.0)\n",
    "    \n",
    "    grad = calculate_gradient(y, tx, w) + (w*lambda_)\n",
    "    \n",
    "    H = calculate_hessian(y, tx, w) + (w.T@w*lambda_ / 2.0)\n",
    "    \n",
    "    \n",
    "    return loss, grad, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # return loss, gradient\n",
    "    loss, grad, H = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    # update w\n",
    "    \n",
    "    w = w - gamma * np.linalg.solve(H, grad);\n",
    "    \n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkappel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 1\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.9\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        #loss, w = learning_by_newton_method(y, tx, w)\n",
    "        #loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w\n",
    "\n",
    "nb = 200\n",
    "\n",
    "x_train2 = x_train[:nb]\n",
    "y_train2 = y_train.reshape(len(y_train), 1)\n",
    "y_train2 = y_train2[:nb]\n",
    "\n",
    "loss, w = logistic_regression_gradient_descent_demo(y_train2, x_train2)\n",
    "#logistic_regression_newton_method_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(nan)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.57085056e-01],\n",
       "       [  1.65317219e-05],\n",
       "       [ -1.33832950e-03],\n",
       "       [ -1.28434808e-03],\n",
       "       [ -1.44225246e-04],\n",
       "       [ -3.55491485e-03],\n",
       "       [  9.27900340e-05],\n",
       "       [ -5.01911896e-03],\n",
       "       [  7.00295081e-02],\n",
       "       [ -1.39055623e-04],\n",
       "       [ -3.42766892e-02],\n",
       "       [ -4.12212280e-02],\n",
       "       [  1.84676417e-02],\n",
       "       [  1.27154137e-02],\n",
       "       [  3.60760574e-02],\n",
       "       [ -7.09694051e-04],\n",
       "       [ -3.50438682e-04],\n",
       "       [  3.68150579e-02],\n",
       "       [  6.94074028e-05],\n",
       "       [ -7.01398203e-04],\n",
       "       [  7.34332123e-04],\n",
       "       [  3.45887833e-04],\n",
       "       [ -7.78494145e-05],\n",
       "       [ -5.00872622e-02],\n",
       "       [ -1.78822212e-04],\n",
       "       [  1.70315202e-04],\n",
       "       [  7.81221814e-05],\n",
       "       [ -3.32517186e-04],\n",
       "       [ -1.71595954e-03],\n",
       "       [ -2.11455686e-03],\n",
       "       [  3.43226511e-02]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68149999999999999"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train2]\n",
    "\n",
    "compare_prediction(w, tx, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
