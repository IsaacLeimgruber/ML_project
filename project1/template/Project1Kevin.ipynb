{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#from implementations import *\n",
    "DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data(DATA_FOLDER + 'train.csv')\n",
    "y_test, x_test, id_test = load_csv_data(DATA_FOLDER + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train.filter(lambda v: v==v, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100000, 100001, 100002, ..., 349997, 349998, 349999])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_prediction(w_train, x, y):\n",
    "    y_pred = predict_labels(w_train, x)\n",
    "    return (y_pred == y).sum()/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "np.shape(w_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74432799999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_ls, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.asarray([1]*30)[:,None].reshape(30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.asarray([1]*30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.asarray([1]*30)\n",
    "max_iters = 30\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkappel/Desktop/EPFL/Master1/ML/Isaac/ML_project/project1/template/loss.py:5: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean(e**2)/2.\n"
     ]
    }
   ],
   "source": [
    "w_sgd, loss_sgd = least_squares_SGD(y_train, x_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62978400000000001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_sgd, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    e = y - tx @ w\n",
    "    return np.mean(e**2)/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "\n",
    "    first_term = tx.T@tx\n",
    "    #sum ici les x*w\n",
    "    left = first_term + lambda_ *np.identity(tx.shape[1])\n",
    "    right = tx.T @ y\n",
    "    w = np.linalg.solve(left, right)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_polynomial import build_poly\n",
    "\n",
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # split data\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    tr_poly = build_poly(x_tr, degree)\n",
    "    te_poly = build_poly(x_te, degree)\n",
    "    #print(np.shape(x_tr))\n",
    "    #print(np.shape(tr_poly))\n",
    "    #####\n",
    "    #tr_poly = np.sum(np.split(tr_poly, degree+1, axis = 1), axis=0)\n",
    "    #te_poly = np.sum(np.split(te_poly, degree+1, axis = 1), axis=0)\n",
    "    #####\n",
    "    \n",
    "    #loss = 1000\n",
    "    best_comp_pred = 0\n",
    "    w = 0\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        # ridge regression\n",
    "        weight, l = ridge_regression(y_tr, tr_poly, lambda_)\n",
    "        #weight = stack_w(weight, degree)\n",
    "        mse_test = compute_loss(y_te, te_poly, weight)\n",
    "        \n",
    "        #print(np.shape(weight))\n",
    "        #print(np.shape(tr_poly))\n",
    "        #print(np.shape(y_tr))\n",
    "        comp_pred = compare_prediction(weight.reshape(len(weight), 1), tr_poly, y_tr.reshape(len(y_tr), 1))\n",
    "        if(comp_pred > best_comp_pred):\n",
    "            best_comp_pred = comp_pred\n",
    "            print(best_comp_pred)\n",
    "            loss = mse_test\n",
    "            w = weight\n",
    "        \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799834482759\n",
      "0.800744827586\n",
      "0.800910344828\n"
     ]
    }
   ],
   "source": [
    "# boucler sur les seeds pour avoir un meileur ? \n",
    "#seed = 95\n",
    "#degree = 6\n",
    "#split_ratio = 0.29\n",
    "\n",
    "seed = 120\n",
    "degree = 6\n",
    "split_ratio = 0.29\n",
    "\n",
    "w_r, loss_ridge = ridge_regression_demo(x_train, y_train, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(w_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_s = np.sum(np.split(w_r, degree + 1),axis=0)\n",
    "np.shape(w_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 180)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(build_poly(x_train, degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80015999999999998"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_r.reshape(len(w_r), 1), build_poly(x_train, degree), y_train.reshape(len(y_train), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression2(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = lamb * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def ridge_regression3(y, tx, lambda_):\n",
    "    first_term = tx.T.dot(tx)\n",
    "    left = first_term + lambda_ *np.identity(tx.shape[1])\n",
    "    right = tx.T.dot(y)\n",
    "    w = np.linalg.solve(left, right)\n",
    "    loss = compute_loss2(y, tx, w)\n",
    "    return w, loss;\n",
    "\n",
    "def compute_loss2(y, tx, w):\n",
    "    e = y - tx.dot(w)\n",
    "    return np.mean(e**2)/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from implementations import least_squares\n",
    "from split_data import split_data\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    num_data = 10000\n",
    "    ratio_train = 0.005\n",
    "    degrees = range(1, 10)\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    \n",
    "    best_seed = 0\n",
    "    best_degree = 0\n",
    "    best_lambda = 0\n",
    "    min_loss = 1000\n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "        # split data with a specific seed\n",
    "        x_tr, y_tr, x_te, y_te = split_data(x_train, y_train, ratio_train, seed)\n",
    "        \n",
    "        # bias_variance_decomposition\n",
    "        for index_deg, degree in enumerate(degrees):\n",
    "            # form data with polynomial degree\n",
    "            train_poly = build_poly(x_tr, degree)\n",
    "            test_poly = build_poly(x_te, degree)\n",
    "            \n",
    "            w_train, loss_train = least_squares(y_tr, train_poly)\n",
    "\n",
    "            # calculate the loss for train and test data\n",
    "            loss_test = compute_loss(y_te, test_poly, w_train)\n",
    "            \n",
    "            if(loss_test < min_loss):\n",
    "                min_loss = loss_test\n",
    "                best_seed = seed\n",
    "                #best_degree = degree\n",
    "                best_lambda = lambda_\n",
    "    #return best_seed, best_lambda, min_loss\n",
    "    return best_seed, best_degree ,min_loss        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#s, d, l = bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_k_indices import build_k_indices\n",
    "from implementations import ridge_regression\n",
    "from build_polynomial import build_poly\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    train_indices = [x for j, x in enumerate(k_indices) if j != k]\n",
    "    train_indices = [idx for part in train_indices for idx in part]\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_tr = x[train_indices]\n",
    "    y_tr = y[train_indices]\n",
    "    \n",
    "    x_te = x[test_indices]\n",
    "    y_te = y[test_indices]\n",
    "\n",
    "    # form data with polynomial degree\n",
    "    #train_poly = build_poly(x_tr, degree)\n",
    "    #test_poly = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression\n",
    "    \n",
    "    weights_train, loss_tr = ridge_regression(y_tr, x_tr, lambda_)\n",
    "\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_te, x_te, weights_train)\n",
    "\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 55\n",
    "    degree = 2\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    \n",
    "    min_mean_loss = 1000\n",
    "    best_lambdas = np.zeros(k_fold)\n",
    "    # cross validation\n",
    "    for i_test in range(k_fold):\n",
    "        min_loss = 1000\n",
    "        for lambda_ in lambdas:\n",
    "            loss_tr, loss_te = cross_validation(y_train, x_train, k_indices, i_test, lambda_, degree)\n",
    "            \n",
    "            if(loss_te < min_loss):\n",
    "                min_loss = loss_te\n",
    "                best_lambdas[i_test] = lambda_\n",
    "    lambda_mean = np.mean(best_lambdas)\n",
    "    return lambda_mean, min_mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bl, mml = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25007499999999999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, l = ridge_regression(y_train, x_train, bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74440799999999996"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    sig = 2*(np.exp(t) / (1 + np.exp(t))) - 1\n",
    "    #sig = (np.exp((t+1)/2) / (np.exp((t+1)/2)+1)\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss2(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    tx_w = tx @ w\n",
    "    \n",
    "    loss = np.log(1 + np.exp(tx_w)) - (y * (tx_w))\n",
    "    \n",
    "    sum_loss = np.sum(loss)\n",
    "    \n",
    "    return sum_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    \n",
    "    return  tx.T @(sigmoid(tx @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shuffle_index_resize(y,x, max_elem_percent):\n",
    "    y_pos = []\n",
    "    y_neg = []\n",
    "    \n",
    "    max_elem = int(((max_elem_percent * len(y) ) / 100) / 2.0)\n",
    "\n",
    "    for idx_y, val in enumerate(y):\n",
    "\n",
    "        if(val == 1):\n",
    "            y_pos.append(idx_y)\n",
    "        else:\n",
    "            y_neg.append(idx_y)\n",
    "\n",
    "        \n",
    "    data_size_pos = len(y_pos)  \n",
    "    shuffle_indices_pos = np.random.permutation(np.arange(data_size_pos)) \n",
    "    data_size_neg = len(y_neg)  \n",
    "    shuffle_indices_neg = np.random.permutation(np.arange(data_size_neg)) \n",
    "    new_y = []\n",
    "    new_x = []\n",
    "    for i in range(max_elem):\n",
    "        new_y.append(y[y_pos[shuffle_indices_pos[i]]])\n",
    "        new_y.append(y[y_neg[shuffle_indices_neg[i]]])\n",
    "        new_x.append(x[y_pos[shuffle_indices_pos[i]]])\n",
    "        new_x.append(x[y_neg[shuffle_indices_neg[i]]])\n",
    "    \n",
    "    return np.array(new_y), np.array(new_x)\n",
    "\n",
    "\n",
    "new_y, new_x = shuffle_index_resize(y_train, x_train, 0.2)\n",
    "\n",
    "np.shape(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient(y, x, w, gamma, max_iter):\n",
    "    # init parameters\n",
    "    #max_iter = 100000\n",
    "    threshold = 1e-8\n",
    "    #gamma = 0.000000001\n",
    "    lambda_ = 0.02\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    #w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter_ in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        #y, x = shuffle_index_resize2(y, x, 50000); #100000 = 40%  12500 = 5%\n",
    "        \n",
    "        if((iter_ % 1000) == 0):\n",
    "            print(iter_)\n",
    "        loss, w = learning_by_gradient_descent(y, x, w, gamma)\n",
    "        \n",
    "        #loss, w = learning_by_newton_method(y, tx, w)\n",
    "        #loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    return loss, w\n",
    "\n",
    "#nb = 200\n",
    "\n",
    "#x_train2 = x_train[:nb]\n",
    "\n",
    "#y_train2 = y_train.reshape(len(y_train), 1)\n",
    "#y_train2 = y_train2[:nb]\n",
    "#loss, w = logistic_regression_gradient_descent_demo(y_train2, x_train2)\n",
    "#loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "29 steps\n",
      "1e-15\n",
      "compare_pred  = 0.640292, best_compare_pred = 0\n",
      "compare_pred = 0.640292\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "28 steps\n",
      "3.29034456231e-15\n",
      "compare_pred  = 0.640596, best_compare_pred = 0.640292\n",
      "compare_pred = 0.640596\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "27 steps\n",
      "1.08263673387e-14\n",
      "compare_pred  = 0.650876, best_compare_pred = 0.640596\n",
      "compare_pred = 0.650876\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "26 steps\n",
      "3.56224789026e-14\n",
      "compare_pred  = 0.681028, best_compare_pred = 0.650876\n",
      "compare_pred = 0.681028\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "25 steps\n",
      "1.17210229753e-13\n",
      "compare_pred  = 0.69362, best_compare_pred = 0.681028\n",
      "compare_pred = 0.693620\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "24 steps\n",
      "3.85662042116e-13\n",
      "compare_pred  = 0.69798, best_compare_pred = 0.69362\n",
      "compare_pred = 0.697980\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "23 steps\n",
      "1.26896100317e-12\n",
      "compare_pred  = 0.703372, best_compare_pred = 0.69798\n",
      "compare_pred = 0.703372\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "22 steps\n",
      "4.17531893656e-12\n",
      "compare_pred  = 0.713932, best_compare_pred = 0.703372\n",
      "compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "21 steps\n",
      "1.37382379588e-11\n",
      "compare_pred  = 0.694568, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "20 steps\n",
      "4.52035365636e-11\n",
      "compare_pred  = 0.701316, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "19 steps\n",
      "1.48735210729e-10\n",
      "compare_pred  = 0.69562, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "18 steps\n",
      "4.89390091848e-10\n",
      "compare_pred  = 0.497568, best_compare_pred = 0.713932\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkappel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/kevinkappel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/kevinkappel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "17 steps\n",
      "1.61026202756e-09\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkappel/Desktop/EPFL/Master1/ML/Isaac/ML_project/project1/template/proj1_helpers.py:30: RuntimeWarning: invalid value encountered in less_equal\n",
      "  y_pred[np.where(y_pred <= 0)] = -1\n",
      "/Users/kevinkappel/Desktop/EPFL/Master1/ML/Isaac/ML_project/project1/template/proj1_helpers.py:31: RuntimeWarning: invalid value encountered in greater\n",
      "  y_pred[np.where(y_pred > 0)] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "16 steps\n",
      "5.29831690628e-09\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "15 steps\n",
      "1.7433288222e-08\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "14 steps\n",
      "5.73615251045e-08\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "13 steps\n",
      "1.88739182214e-07\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "12 steps\n",
      "6.21016941892e-07\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "11 steps\n",
      "2.04335971786e-06\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "10 steps\n",
      "6.7233575365e-06\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "9 steps\n",
      "2.21221629107e-05\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "8 steps\n",
      "7.27895384398e-05\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "7 steps\n",
      "0.000239502661999\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "6 steps\n",
      "0.000788046281567\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5 steps\n",
      "0.0025929437974\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "4 steps\n",
      "0.00853167852417\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "3 steps\n",
      "0.0280721620394\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "2 steps\n",
      "0.0923670857187\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "1 steps\n",
      "0.303919538231\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "0 steps\n",
      "1.0\n",
      "compare_pred  = 0.0, best_compare_pred = 0.713932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test():\n",
    "    #gammas = np.arange(0.48e-13,0.51e-13,1e-15)\n",
    "    gammas = np.logspace(-15, 0, 30)\n",
    "    # last = 5.5e-12\n",
    "\n",
    "    max_iter = 5000\n",
    "    #max_iter = 100000\n",
    "\n",
    "    best_compare_pred = 0\n",
    "    best_gamma = 0\n",
    "    best_w = 0\n",
    "\n",
    "\n",
    "    #y_train2, x_train2 = shuffle_index_resize(y_train,x_train, 68)\n",
    "    x_train2 = x_train\n",
    "    y_train2 = y_train\n",
    "\n",
    "    y_train2 = y_train2.reshape(len(y_train2), 1)\n",
    "    #print(np.shape(y_train2))\n",
    "    #print(np.shape(x_train2))\n",
    "\n",
    "\n",
    "    tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train2]\n",
    "    w_init = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    #print(np.shape(w_init))\n",
    "\n",
    "    gamma_step = len(gammas)\n",
    "\n",
    "    for gamma_ in gammas:\n",
    "    \n",
    "        loss, w = logistic_regression_gradient(y_train2, tx, w_init, gamma_, max_iter)\n",
    "    \n",
    "        #print(np.shape(w))\n",
    "        gamma_step -= 1;\n",
    "        print('%d steps' % gamma_step)\n",
    "        print(gamma_)\n",
    "    \n",
    "        compare_pred = compare_prediction(w, tx, y_train2)\n",
    "        print('compare_pred  = ' + str(compare_pred) + ', best_compare_pred = '+ str(best_compare_pred))\n",
    "      \n",
    "        if(compare_pred > best_compare_pred):\n",
    "            print('compare_pred = %f' % compare_pred)\n",
    "            best_compare_pred = compare_pred;\n",
    "            best_gamma = gamma_;\n",
    "            best_w = w;\n",
    "        \n",
    "    return best_compare_pred, best_gamma, best_w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.8999255836\n",
      "4.9e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_compare_pred, best_gamma, best_w  = test()\n",
    "print(best_compare_pred)\n",
    "print(best_gamma)\n",
    "print(np.shape(best_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69247599999999998"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(best_w, tx, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_w_saved = best_w\n",
    "best_gamma_saved = best_gamma\n",
    "best_loss_saved = best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    # return loss, gradient, and hessian\n",
    "    #loss = calculate_loss(y, tx, w) + (w.T@w*lambda_ / 2.0)\n",
    "    \n",
    "    grad = calculate_gradient(y, tx, w) + (w*lambda_)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shuffle_index_resize2(y,x, max_elem_percent):\n",
    "    y_pos = []\n",
    "    y_neg = []\n",
    "    \n",
    "    max_elem = int(max_elem_percent / 2)\n",
    "\n",
    "    for idx_y, val in enumerate(y):\n",
    "\n",
    "        if(val == 1):\n",
    "            y_pos.append(idx_y)\n",
    "        else:\n",
    "            y_neg.append(idx_y)\n",
    "\n",
    "        \n",
    "    data_size_pos = len(y_pos)  \n",
    "    shuffle_indices_pos = np.random.permutation(np.arange(data_size_pos)) \n",
    "    data_size_neg = len(y_neg)  \n",
    "    shuffle_indices_neg = np.random.permutation(np.arange(data_size_neg)) \n",
    "    new_y = []\n",
    "    new_x = []\n",
    "    for i in range(max_elem):\n",
    "        new_y.append(y[y_pos[shuffle_indices_pos[i]]])\n",
    "        new_y.append(y[y_neg[shuffle_indices_neg[i]]])\n",
    "        new_x.append(x[y_pos[shuffle_indices_pos[i]]])\n",
    "        new_x.append(x[y_neg[shuffle_indices_neg[i]]])\n",
    "    \n",
    "    return np.array(new_y), np.array(new_x)\n",
    "\n",
    "\n",
    "new_y, new_x = shuffle_index_resize2(y_train, x_train, 0.2)\n",
    "\n",
    "np.shape(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x, w, gamma, max_iter, lambda_):\n",
    "    # init parameters\n",
    "    #max_iter = 100000\n",
    "    #gamma = 0.000000001\n",
    "    #lambda_ = 0.02\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "        \n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    #w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter_ in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        y, x = shuffle_index_resize2(y, x, 50000); #100000 = 40%  12500 = 5%\n",
    "        \n",
    "        if((iter_ % 100) == 0):\n",
    "            print(iter_)\n",
    "        w = learning_by_penalized_gradient(y, x, w, gamma, lambda_)\n",
    "        \n",
    "        #losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            #break\n",
    "        \n",
    "        #loss, w = learning_by_newton_method(y, tx, w)\n",
    "        #loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    return w\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    \n",
    "    # problem ici avec 1 et sum\n",
    "    phi_x = []\n",
    "    \n",
    "    for j in range(0, degree + 1):\n",
    "        if(j == 0):\n",
    "            phi_x = np.power(x, j)\n",
    "        else:\n",
    "            x_power_j = np.power(x, j)\n",
    "            phi_x = np.c_[phi_x, x_power_j]\n",
    "\n",
    "    return phi_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-088b09b6fa09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_gradient_descent_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#comp_pred = compare_prediction(weight.reshape(len(weight), 1), tr_poly, y_tr.reshape(len(y_tr), 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-a2ab16d1c406>\u001b[0m in \u001b[0;36mlogistic_regression_gradient_descent_demo\u001b[0;34m(y, x, w, gamma, max_iter, lambda_)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_index_resize2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m#100000 = 40%  12500 = 5%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-66ade5365f78>\u001b[0m in \u001b[0;36mshuffle_index_resize2\u001b[0;34m(y, x, max_elem_percent)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnew_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-15, -10, 5)\n",
    "#gammas = np.arange(0.48e-13,1.0e-13,1e-15)\n",
    "# last = 7.19685673001e-12  1e-13\n",
    "\n",
    "max_iter = 400\n",
    "#max_iter = 100000\n",
    "\n",
    "lambdas = np.logspace(-20, -15, 5)\n",
    "#lambdas = np.logspace(-13, -10, 2)\n",
    "#lambdas = np.arange(1.9e-9,2.1e-9,1e-10)\n",
    "# last =\n",
    "\n",
    "best_comp_pred = 0\n",
    "best_gamma = 0\n",
    "best_w = 0\n",
    "best_lambda = 0\n",
    "\n",
    "\n",
    "#degree = 4\n",
    "#x_train2 = build_poly(x_train, degree)\n",
    "x_train2 = x_train\n",
    "y_train2 = y_train\n",
    "#y_train2, x_train2 = shuffle_index_resize(y_train,x_train, 60)\n",
    "\n",
    "y_train2 = y_train2.reshape(len(y_train2), 1)\n",
    "\n",
    "tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train2]\n",
    "w_init = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "gamma_step = len(gammas)\n",
    "\n",
    "for gamma_ in gammas:\n",
    "    lambda_step = len(lambdas)\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        w = logistic_regression_gradient_descent_demo(y_train2, tx, w_init, gamma_, max_iter, lambda_)\n",
    "        \n",
    "        #comp_pred = compare_prediction(weight.reshape(len(weight), 1), tr_poly, y_tr.reshape(len(y_tr), 1))\n",
    "        \n",
    "        #loss, w = logistic_regression_gradient_descent_demo(y_train2, tx, w_init, gamma_, max_iter, lambda_)\n",
    "        print('degree')\n",
    "        #compare_prediction(w_r.reshape(len(w_r), 1), build_poly(x_train, degree), y_train.reshape(len(y_train), 1))\n",
    "\n",
    "        comp_pred = compare_prediction(w, tx, y_train2)\n",
    "        print('comp_pred  = ' + str(comp_pred) + ', best_comp_pred = '+ str(best_comp_pred))\n",
    "        \n",
    "        if(comp_pred > best_comp_pred):\n",
    "            print('comp_pred = %f' % comp_pred)\n",
    "            best_comp_pred = comp_pred;\n",
    "            best_gamma = gamma_;\n",
    "            best_w = w;\n",
    "            best_lambda = lambda_\n",
    "            \n",
    "        lambda_step -= 1;\n",
    "        print('%d steps_lambda' % lambda_step)\n",
    "        print('lambda')\n",
    "        print(lambda_)\n",
    "        \n",
    "        #comp_pred = compare_prediction(w, tx, y_train2)\n",
    "        #print('comp_pred  = ' + str(comp_pred) + ', best_comp_pred = '+ str(best_comp_pred))\n",
    "        \n",
    "        #if(comp_pred > best_comp_pred):\n",
    "            #print('comp_pred = %f' % comp_pred)\n",
    "            #best_comp_pred = comp_pred;\n",
    "            #best_gamma = gamma_;\n",
    "            #best_w = w;\n",
    "            #best_lambda = lambda_\n",
    "    \n",
    "    #print(np.shape(w))\n",
    "    gamma_step -= 1;\n",
    "    print('%d steps_gamma' % gamma_step)\n",
    "    print('gamma')\n",
    "    print(gamma_)\n",
    "        \n",
    "best_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_loss)\n",
    "print(best_gamma)\n",
    "np.shape(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68149999999999999"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(best_w, tx, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_w2_saved = best_w\n",
    "best_gamma2_saved = best_gamma\n",
    "best_loss2_saved = best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, gamma_, max_iter):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    train_indices = [x for j, x in enumerate(k_indices) if j != k]\n",
    "    train_indices = [idx for part in train_indices for idx in part]\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    y_train2 = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "    x_train2 = np.c_[np.ones((y_train2.shape[0], 1)), x_train]\n",
    "    w_init = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # Logistic regression\n",
    "    \n",
    "    #print(np.shape(y_train2))\n",
    "    #print(np.shape(x_train2))\n",
    "    \n",
    "    #weights_train, loss_tr = ridge_regression(y_train, train_poly, lambda_)\n",
    "    loss_tr, weights_train = logistic_regression_gradient_descent_demo(y_train2, x_train2, w_init, gamma_, max_iter, lambda_)\n",
    "    \n",
    "    return loss_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "14 steps_lambda\n",
      "lambda\n",
      "1e-13\n",
      "lambda_mean\n",
      "241.911853099\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "13 steps_lambda\n",
      "lambda\n",
      "8.48342898244e-13\n",
      "lambda_mean\n",
      "241.339107838\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "12 steps_lambda\n",
      "lambda\n",
      "7.19685673001e-12\n",
      "lambda_mean\n",
      "241.135487565\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "11 steps_lambda\n",
      "lambda\n",
      "6.10540229659e-11\n",
      "lambda_mean\n",
      "241.121210965\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "10 steps_lambda\n",
      "lambda\n",
      "5.17947467923e-10\n",
      "lambda_mean\n",
      "241.271827555\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "9 steps_lambda\n",
      "lambda\n",
      "4.39397056076e-09\n",
      "lambda_mean\n",
      "241.208487777\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "8 steps_lambda\n",
      "lambda\n",
      "3.72759372031e-08\n",
      "lambda_mean\n",
      "241.183545198\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "loss  = 29503.2235826, best_loss = 10000\n",
      "0\n",
      "100\n",
      "200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f3aefddf15a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mcross_validation_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-f3aefddf15a2>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mloss_tr_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss  = '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', best_loss = '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-50425659bde7>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, lambda_, gamma_, max_iter)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#weights_train, loss_tr = ridge_regression(y_train, train_poly, lambda_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_gradient_descent_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-8e07de8b9e81>\u001b[0m in \u001b[0;36mlogistic_regression_gradient_descent_demo\u001b[0;34m(y, x, w, gamma, max_iter, lambda_)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_index_resize2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m#100000 = 40%  12500 = 5%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a8768dd4c4e4>\u001b[0m in \u001b[0;36mshuffle_index_resize2\u001b[0;34m(y, x, max_elem_percent)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0my_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    k_fold = 2\n",
    "    max_iter = 400\n",
    "    lambdas = np.logspace(-13, 0, 15)\n",
    "    gammas = np.logspace(-13, 0, 15)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    best_loss_train = 100000\n",
    "    best_gamma = 0\n",
    "    best_lambda = 0\n",
    "    #best_w = 0\n",
    "    \n",
    "    gamma_step = len(gammas)\n",
    "    \n",
    "    # cross validation\n",
    "    for gamma_ in gammas:\n",
    "        loss_tr_tmp = []\n",
    "        lambda_step = len(lambdas)\n",
    "        \n",
    "        for lambda_ in lambdas: \n",
    "            \n",
    "            for i_test in range(k_fold): \n",
    "                loss_tr = cross_validation(y_train, x_train, k_indices, i_test, lambda_, gamma_, max_iter)\n",
    "                loss_tr_tmp.append(loss_tr)\n",
    "                print('loss  = ' + str(loss) + ', best_loss = '+ str(best_loss_train))\n",
    "            \n",
    "            lambda_step -= 1;\n",
    "            print('%d steps_lambda' % lambda_step)\n",
    "            print('lambda')\n",
    "            print(lambda_)\n",
    "            print('loss_mean')\n",
    "            print(np.sqrt(2*np.mean(loss_tr_tmp)))\n",
    "            \n",
    "            #compare_pred = compare_prediction(best_w, tx, y_train2)\n",
    "            \n",
    "            loss_train = np.sqrt(2*np.mean(loss_tr_tmp))\n",
    "            \n",
    "            if(loss_train < best_loss_train):\n",
    "                \n",
    "                best_loss_train = loss_train\n",
    "                best_gamma = gamma_\n",
    "                best_lambda = lambda_\n",
    "                #best_w = w  \n",
    "        gamma_step -= 1;\n",
    "        print('%d steps_gamma' % gamma_step)\n",
    "        print('gamma')\n",
    "        print(gamma_)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e-13,   1.66088278e-13,   2.75853162e-13,\n",
       "         4.58159767e-13,   7.60949669e-13,   1.26384820e-12,\n",
       "         2.09910372e-12,   3.48636523e-12,   5.79044398e-12,\n",
       "         9.61724871e-12,   1.59731228e-11,   2.65294846e-11,\n",
       "         4.40623643e-11,   7.31824222e-11,   1.21547425e-10,\n",
       "         2.01876025e-10,   3.35292415e-10,   5.56881399e-10,\n",
       "         9.24914728e-10,   1.53617495e-09,   2.55140652e-09,\n",
       "         4.23758716e-09,   7.03813555e-09,   1.16895182e-08,\n",
       "         1.94149195e-08,   3.22459055e-08,   5.35566692e-08,\n",
       "         8.89513497e-08,   1.47737765e-07,   2.45375111e-07,\n",
       "         4.07539297e-07,   6.76875001e-07,   1.12421004e-06,\n",
       "         1.86718109e-06,   3.10116893e-06,   5.15067808e-06,\n",
       "         8.55467254e-06,   1.42083083e-05,   2.35983347e-05,\n",
       "         3.91940677e-05,   6.50967523e-05,   1.08118075e-04,\n",
       "         1.79571449e-04,   2.98247129e-04,   4.95353521e-04,\n",
       "         8.22724134e-04,   1.36644835e-03,   2.26951054e-03,\n",
       "         3.76939098e-03,   6.26051657e-03,   1.03979842e-02,\n",
       "         1.72698329e-02,   2.86831681e-02,   4.76393801e-02,\n",
       "         7.91234262e-02,   1.31414736e-01,   2.18264473e-01,\n",
       "         3.62511705e-01,   6.02089449e-01,   1.00000000e+00])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_loss)\n",
    "print(best_gamma)\n",
    "np.shape(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train2 = y_train.reshape(len(y_train), 1)\n",
    "tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train]\n",
    "\n",
    "compare_prediction(best_w, tx, y_train2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
