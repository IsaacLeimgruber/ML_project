{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#from implementations import *\n",
    "DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data(DATA_FOLDER + 'train.csv')\n",
    "y_test, x_test, id_test = load_csv_data(DATA_FOLDER + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_prediction(w_train, x, y):\n",
    "    y_pred = predict_labels(w_train, x)\n",
    "    return (y_pred == y).sum()/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74432799999999999"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "compare_prediction(w_ls, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-306-3fe24d9c5fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw_ls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_ls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_poly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcompare_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_ls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_poly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ML_project\\project1\\template\\implementations.py\u001b[0m in \u001b[0;36mleast_squares\u001b[1;34m(y, tx)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;34m\"\"\"calculate the least squares solution.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36msolve\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DD->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'dd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "x_poly = build_poly(x_train[:2000], 4)\n",
    "w_ls, loss_ls = least_squares(y_train[:2000], x_poly)\n",
    "compare_prediction(w_ls, x_poly, y_train[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.asarray([1]*30)\n",
    "max_iters = 30\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ML_project\\project1\\template\\loss.py:5: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean(e**2)/2.\n"
     ]
    }
   ],
   "source": [
    "w_sgd, loss_sgd = least_squares_SGD(y_train, x_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63045200000000001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_sgd, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_polynomial import build_poly\n",
    "\n",
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "    # split data\n",
    "    x_tr, y_tr, x_te, y_te = split_data(x, y, ratio, seed)\n",
    "    \n",
    "    tr_poly = build_poly(x_tr, degree)\n",
    "    te_poly = build_poly(x_te, degree)\n",
    "    \n",
    "    #loss = 1000\n",
    "    best_comp_pred = 0\n",
    "    w = 0\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        # ridge regression\n",
    "        weight, l = ridge_regression(y_tr, tr_poly, lambda_, 1)\n",
    "        #weight = stack_w(weight, degree)\n",
    "        mse_test = compute_loss(y_te, te_poly, weight)\n",
    "        \n",
    "        comp_pred = compare_prediction(weight.reshape(len(weight), 1), tr_poly, y_tr.reshape(len(y_tr), 1))\n",
    "        if(comp_pred > best_comp_pred):\n",
    "            best_comp_pred = comp_pred\n",
    "            print(best_comp_pred)\n",
    "            loss = mse_test\n",
    "            w = weight\n",
    "        \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799782222222\n",
      "0.800048888889\n"
     ]
    }
   ],
   "source": [
    "# boucler sur les seeds pour avoir un meileur ? \n",
    "#seed = 95\n",
    "#degree = 6\n",
    "#split_ratio = 0.29\n",
    "\n",
    "seed = 120\n",
    "degree = 6\n",
    "split_ratio = 0.9\n",
    "\n",
    "w_r, loss_ridge = ridge_regression_demo(x_train, y_train, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.799736"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_prediction(w_r.reshape(len(w_r), 1), build_poly(x_train, degree), y_train.reshape(len(y_train), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_k_indices import build_k_indices\n",
    "from implementations import ridge_regression\n",
    "from build_polynomial import build_poly\n",
    "\n",
    "def cross_validation(func, y, x, k_indices, k, lambda_, degree, gamma):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    train_indices = [x for j, x in enumerate(k_indices) if j != k]\n",
    "    train_indices = [idx for part in train_indices for idx in part]\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_tr = x[train_indices]\n",
    "    y_tr = y[train_indices]\n",
    "    \n",
    "    x_te = x[test_indices]\n",
    "    y_te = y[test_indices]\n",
    "\n",
    "    # form data with polynomial degree\n",
    "    train_poly = build_poly(x_tr, degree)\n",
    "    test_poly = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression\n",
    "    weights_train, loss_tr = func(y_tr, train_poly, lambda_, gamma)\n",
    "\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_te, test_poly, weights_train)\n",
    "\n",
    "    return loss_tr, loss_te, weights_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_valid_vizu(values, name, rmse_tr, rmse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(values, rmse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(values, rmse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(name)\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"cross_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_demo(func, lambdas, degrees, gammas, k_fold):\n",
    "    \n",
    "    seed = 1\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train[:4000], k_fold, seed)\n",
    "    \n",
    "    best_l = 0 \n",
    "    best_deg = 0\n",
    "    best_g = 0\n",
    "    min_loss = 1000\n",
    "    best_pred = 0\n",
    "     # define list to store the variable\n",
    "    rmse_tr = np.empty((len(lambdas), len(degrees), len(gammas)))\n",
    "    rmse_te = np.empty((len(lambdas), len(degrees), len(gammas)))\n",
    "    \n",
    "    for ind_l, l in enumerate(lambdas):\n",
    "        print(ind_l)\n",
    "        for ind_deg, deg in enumerate(degrees):\n",
    "            for ind_g, g in enumerate(gammas):\n",
    "                sum_loss_tr = 0\n",
    "                sum_loss_te = 0\n",
    "                for k_test in range(k_fold):\n",
    "                    loss_tr, loss_te, we = cross_validation(func, y_train[:100000], x_train[:100000], k_indices, k_test, l, deg, g)\n",
    "                    sum_loss_tr = sum_loss_tr + loss_tr\n",
    "                    sum_loss_te = sum_loss_te + loss_te\n",
    "                rmse_t = np.sqrt(2*sum_loss_te)\n",
    "                rmse_tr[ind_l][ind_deg][ind_g] = np.sqrt(2*sum_loss_tr)\n",
    "                rmse_te[ind_l][ind_deg][ind_g] = rmse_t\n",
    "                \n",
    "                #pred = compare_prediction(we, build_poly(x_train, deg), y_train)\n",
    "                #if(pred > best_pred):\n",
    "                #    best_pred = pred\n",
    "                if(rmse_t < min_loss):\n",
    "                    min_loss = rmse_t\n",
    "                    best_l = l\n",
    "                    best_deg = deg\n",
    "                    best_g = g\n",
    "    print(np.argmin(rmse_te))\n",
    "    print(np.shape(rmse_tr[:, 0, 0]))\n",
    "    cross_valid_vizu(lambdas, 'lambdas', rmse_tr[:, 0, 0], rmse_te[:, 0, 0])\n",
    "    return best_l, best_deg, best_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    seed = 69\n",
    "    degree = 2\n",
    "    degrees = range(6, 7)\n",
    "    k_fold = 4\n",
    "    lambdas = [1]#np.logspace(-5, 0, 30)\n",
    "    gammas_ridge = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2122162910704501e-05"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "2\n",
      "(30,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1IvRjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24f573257f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bl, bd, bg = cross_validation_demo(ridge_regression, lambdas, degrees, gammas_ridge, k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2122162910704501e-05"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_poly = build_poly(x_train, 6)\n",
    "epsylon = 10.**(-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79975200000000002"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, l = ridge_regression(y_train, x_poly, bl, 1)\n",
    "compare_prediction(w, x_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79975200000000002"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, l = ridge_regression(y_train, x_poly, bl+epsylon, 1)\n",
    "compare_prediction(w, x_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79969999999999997"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, l = ridge_regression(y_train, x_poly, bl-epsylon, 1)\n",
    "compare_prediction(w, x_poly, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_logi(y, x, k_indices, k, lambda_, degree, gamma):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    train_indices = [x for j, x in enumerate(k_indices) if j != k]\n",
    "    train_indices = [idx for part in train_indices for idx in part]\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_tr = x[train_indices]\n",
    "    y_tr = y[train_indices]\n",
    "    \n",
    "    x_te = x[test_indices]\n",
    "    y_te = y[test_indices]\n",
    "\n",
    "    # form data with polynomial degree\n",
    "    train_poly = build_poly(x_tr, degree)\n",
    "    test_poly = build_poly(x_te, degree)\n",
    "\n",
    "    # ridge regression\n",
    "    weights_train, loss_tr = logistic_regression(y_tr, train_poly, lambda_, gamma)\n",
    "\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_te, test_poly, weights_train)\n",
    "\n",
    "    return loss_tr, loss_te, weights_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_logi_demo(lambdas, degrees, gammas, k_fold):\n",
    "    \n",
    "    seed = 1\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train[:4000], k_fold, seed)\n",
    "    \n",
    "    best_l = 0 \n",
    "    best_deg = 0\n",
    "    best_g = 0\n",
    "    min_loss = 1000\n",
    "    best_pred = 0\n",
    "     # define list to store the variable\n",
    "    rmse_tr = np.empty((len(lambdas), len(degrees), len(gammas)))\n",
    "    rmse_te = np.empty((len(lambdas), len(degrees), len(gammas)))\n",
    "    \n",
    "    for ind_l, l in enumerate(lambdas):\n",
    "        print(ind_l)\n",
    "        for ind_deg, deg in enumerate(degrees):\n",
    "            for ind_g, g in enumerate(gammas):\n",
    "                sum_loss_tr = 0\n",
    "                sum_loss_te = 0\n",
    "                for k_test in range(k_fold):\n",
    "                    loss_tr, loss_te, we = cross_validation_logi(y_train[:100000],  \\\n",
    "                                                            x_train[:100000], k_indices, k_test, l, deg, g)\n",
    "                    sum_loss_tr = sum_loss_tr + loss_tr\n",
    "                    sum_loss_te = sum_loss_te + loss_te\n",
    "                rmse_t = np.sqrt(2*sum_loss_te)\n",
    "                rmse_tr[ind_l][ind_deg][ind_g] = np.sqrt(2*sum_loss_tr)\n",
    "                rmse_te[ind_l][ind_deg][ind_g] = rmse_t\n",
    "                \n",
    "                #pred = compare_prediction(we, build_poly(x_train, deg), y_train)\n",
    "                #if(pred > best_pred):\n",
    "                #    best_pred = pred\n",
    "                if(rmse_t < min_loss):\n",
    "                    min_loss = rmse_t\n",
    "                    best_l = l\n",
    "                    best_deg = deg\n",
    "                    best_g = g\n",
    "                    \n",
    "    cross_valid_vizu(lambdas, 'lambdas', rmse_tr[:, 0, 0], rmse_te[:, 0, 0])\n",
    "    return best_l, best_deg, best_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " degrees = range(6, 7)\n",
    "    k_fold = 4\n",
    "    lambdas = [1]#np.logspace(-5, 0, 30)\n",
    "    gammas_ridge = [1]#np.logspace(-15, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bl, bd, bg = cross_validation_logi_demo(lambdas, degrees, gammas_ridge, k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END CROSS_VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    sig = 2*(np.exp(t) / (1 + np.exp(t))) - 1\n",
    "    #sig = (np.exp((t+1)/2) / (np.exp((t+1)/2)+1)\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    \n",
    "    return  tx.T @(sigmoid(tx @ w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_index_resize(y,x, max_elem_percent):\n",
    "    y_pos = []\n",
    "    y_neg = []\n",
    "    \n",
    "    max_elem = int(((max_elem_percent * len(y) ) / 100) / 2.0)\n",
    "\n",
    "    for idx_y, val in enumerate(y):\n",
    "\n",
    "        if(val == 1):\n",
    "            y_pos.append(idx_y)\n",
    "        else:\n",
    "            y_neg.append(idx_y)\n",
    "\n",
    "        \n",
    "    data_size_pos = len(y_pos)  \n",
    "    shuffle_indices_pos = np.random.permutation(np.arange(data_size_pos)) \n",
    "    data_size_neg = len(y_neg)  \n",
    "    shuffle_indices_neg = np.random.permutation(np.arange(data_size_neg)) \n",
    "    new_y = []\n",
    "    new_x = []\n",
    "    for i in range(max_elem):\n",
    "        new_y.append(y[y_pos[shuffle_indices_pos[i]]])\n",
    "        new_y.append(y[y_neg[shuffle_indices_neg[i]]])\n",
    "        new_x.append(x[y_pos[shuffle_indices_pos[i]]])\n",
    "        new_x.append(x[y_neg[shuffle_indices_neg[i]]])\n",
    "    \n",
    "    return np.array(new_y), np.array(new_x)\n",
    "\n",
    "\n",
    "new_y, new_x = shuffle_index_resize(y_train, x_train, 0.2)\n",
    "\n",
    "np.shape(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient(y, x, w, gamma, max_iter):\n",
    "    # init parameters\n",
    "    #max_iter = 100000\n",
    "    threshold = 1e-8\n",
    "    #gamma = 0.000000001\n",
    "    lambda_ = 0.02\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    #w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter_ in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        #y, x = shuffle_index_resize2(y, x, 50000); #100000 = 40%  12500 = 5%\n",
    "        \n",
    "        if((iter_ % 1000) == 0):\n",
    "            print(iter_)\n",
    "        loss, w = learning_by_gradient_descent(y, x, w, gamma)\n",
    "        \n",
    "        #loss, w = learning_by_newton_method(y, tx, w)\n",
    "        #loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    return loss, w\n",
    "\n",
    "#nb = 200\n",
    "\n",
    "#x_train2 = x_train[:nb]\n",
    "\n",
    "#y_train2 = y_train.reshape(len(y_train), 1)\n",
    "#y_train2 = y_train2[:nb]\n",
    "#loss, w = logistic_regression_gradient_descent_demo(y_train2, x_train2)\n",
    "#loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    #gammas = np.arange(0.48e-13,0.51e-13,1e-15)\n",
    "    gammas = np.logspace(-15, 0, 30)\n",
    "    # last = 5.5e-12\n",
    "\n",
    "    max_iter = 5000\n",
    "    #max_iter = 100000\n",
    "\n",
    "    best_compare_pred = 0\n",
    "    best_gamma = 0\n",
    "    best_w = 0\n",
    "\n",
    "\n",
    "    #y_train2, x_train2 = shuffle_index_resize(y_train,x_train, 68)\n",
    "    x_train2 = x_train\n",
    "    y_train2 = y_train\n",
    "\n",
    "    y_train2 = y_train2.reshape(len(y_train2), 1)\n",
    "    #print(np.shape(y_train2))\n",
    "    #print(np.shape(x_train2))\n",
    "\n",
    "\n",
    "    tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train2]\n",
    "    w_init = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    #print(np.shape(w_init))\n",
    "\n",
    "    gamma_step = len(gammas)\n",
    "\n",
    "    for gamma_ in gammas:\n",
    "    \n",
    "        loss, w = logistic_regression_gradient(y_train2, tx, w_init, gamma_, max_iter)\n",
    "    \n",
    "        #print(np.shape(w))\n",
    "        gamma_step -= 1;\n",
    "        print('%d steps' % gamma_step)\n",
    "        print(gamma_)\n",
    "    \n",
    "        compare_pred = compare_prediction(w, tx, y_train2)\n",
    "        print('compare_pred  = ' + str(compare_pred) + ', best_compare_pred = '+ str(best_compare_pred))\n",
    "      \n",
    "        if(compare_pred > best_compare_pred):\n",
    "            print('compare_pred = %f' % compare_pred)\n",
    "            best_compare_pred = compare_pred;\n",
    "            best_gamma = gamma_;\n",
    "            best_w = w;\n",
    "        \n",
    "    return best_compare_pred, best_gamma, best_w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_compare_pred, best_gamma, best_w  = test()\n",
    "print(best_compare_pred)\n",
    "print(best_gamma)\n",
    "print(np.shape(best_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_prediction(best_w, tx, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_w_saved = best_w\n",
    "best_gamma_saved = best_gamma\n",
    "best_loss_saved = best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    # return loss, gradient, and hessian\n",
    "    #loss = calculate_loss(y, tx, w) + (w.T@w*lambda_ / 2.0)\n",
    "    \n",
    "    grad = calculate_gradient(y, tx, w) + (w*lambda_)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_index_resize2(y,x, max_elem_percent):\n",
    "    y_pos = []\n",
    "    y_neg = []\n",
    "    \n",
    "    max_elem = int(max_elem_percent / 2)\n",
    "\n",
    "    for idx_y, val in enumerate(y):\n",
    "\n",
    "        if(val == 1):\n",
    "            y_pos.append(idx_y)\n",
    "        else:\n",
    "            y_neg.append(idx_y)\n",
    "\n",
    "        \n",
    "    data_size_pos = len(y_pos)  \n",
    "    shuffle_indices_pos = np.random.permutation(np.arange(data_size_pos)) \n",
    "    data_size_neg = len(y_neg)  \n",
    "    shuffle_indices_neg = np.random.permutation(np.arange(data_size_neg)) \n",
    "    new_y = []\n",
    "    new_x = []\n",
    "    for i in range(max_elem):\n",
    "        new_y.append(y[y_pos[shuffle_indices_pos[i]]])\n",
    "        new_y.append(y[y_neg[shuffle_indices_neg[i]]])\n",
    "        new_x.append(x[y_pos[shuffle_indices_pos[i]]])\n",
    "        new_x.append(x[y_neg[shuffle_indices_neg[i]]])\n",
    "    \n",
    "    return np.array(new_y), np.array(new_x)\n",
    "\n",
    "\n",
    "new_y, new_x = shuffle_index_resize2(y_train, x_train, 0.2)\n",
    "\n",
    "np.shape(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, x, w, gamma, max_iter, lambda_):\n",
    "    # init parameters\n",
    "    #max_iter = 100000\n",
    "    #gamma = 0.000000001\n",
    "    #lambda_ = 0.02\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "        \n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    #w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter_ in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        y, x = shuffle_index_resize2(y, x, 50000); #100000 = 40%  12500 = 5%\n",
    "        \n",
    "        if((iter_ % 100) == 0):\n",
    "            print(iter_)\n",
    "        w = learning_by_penalized_gradient(y, x, w, gamma, lambda_)\n",
    "        \n",
    "        #losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            #break\n",
    "        \n",
    "        #loss, w = learning_by_newton_method(y, tx, w)\n",
    "        #loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    return w\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    \n",
    "    # problem ici avec 1 et sum\n",
    "    phi_x = []\n",
    "    \n",
    "    for j in range(0, degree + 1):\n",
    "        if(j == 0):\n",
    "            phi_x = np.power(x, j)\n",
    "        else:\n",
    "            x_power_j = np.power(x, j)\n",
    "            phi_x = np.c_[phi_x, x_power_j]\n",
    "\n",
    "    return phi_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas = np.logspace(-15, -10, 5)\n",
    "#gammas = np.arange(0.48e-13,1.0e-13,1e-15)\n",
    "# last = 7.19685673001e-12  1e-13\n",
    "\n",
    "max_iter = 400\n",
    "#max_iter = 100000\n",
    "\n",
    "lambdas = np.logspace(-20, -15, 5)\n",
    "#lambdas = np.logspace(-13, -10, 2)\n",
    "#lambdas = np.arange(1.9e-9,2.1e-9,1e-10)\n",
    "# last =\n",
    "\n",
    "best_comp_pred = 0\n",
    "best_gamma = 0\n",
    "best_w = 0\n",
    "best_lambda = 0\n",
    "\n",
    "\n",
    "#degree = 4\n",
    "#x_train2 = build_poly(x_train, degree)\n",
    "x_train2 = x_train\n",
    "y_train2 = y_train\n",
    "#y_train2, x_train2 = shuffle_index_resize(y_train,x_train, 60)\n",
    "\n",
    "y_train2 = y_train2.reshape(len(y_train2), 1)\n",
    "\n",
    "tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train2]\n",
    "w_init = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "gamma_step = len(gammas)\n",
    "\n",
    "for gamma_ in gammas:\n",
    "    lambda_step = len(lambdas)\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        w = logistic_regression_gradient_descent_demo(y_train2, tx, w_init, gamma_, max_iter, lambda_)\n",
    "        \n",
    "        #comp_pred = compare_prediction(weight.reshape(len(weight), 1), tr_poly, y_tr.reshape(len(y_tr), 1))\n",
    "        \n",
    "        #loss, w = logistic_regression_gradient_descent_demo(y_train2, tx, w_init, gamma_, max_iter, lambda_)\n",
    "        print('degree')\n",
    "        #compare_prediction(w_r.reshape(len(w_r), 1), build_poly(x_train, degree), y_train.reshape(len(y_train), 1))\n",
    "\n",
    "        comp_pred = compare_prediction(w, tx, y_train2)\n",
    "        print('comp_pred  = ' + str(comp_pred) + ', best_comp_pred = '+ str(best_comp_pred))\n",
    "        \n",
    "        if(comp_pred > best_comp_pred):\n",
    "            print('comp_pred = %f' % comp_pred)\n",
    "            best_comp_pred = comp_pred;\n",
    "            best_gamma = gamma_;\n",
    "            best_w = w;\n",
    "            best_lambda = lambda_\n",
    "            \n",
    "        lambda_step -= 1;\n",
    "        print('%d steps_lambda' % lambda_step)\n",
    "        print('lambda')\n",
    "        print(lambda_)\n",
    "        \n",
    "        #comp_pred = compare_prediction(w, tx, y_train2)\n",
    "        #print('comp_pred  = ' + str(comp_pred) + ', best_comp_pred = '+ str(best_comp_pred))\n",
    "        \n",
    "        #if(comp_pred > best_comp_pred):\n",
    "            #print('comp_pred = %f' % comp_pred)\n",
    "            #best_comp_pred = comp_pred;\n",
    "            #best_gamma = gamma_;\n",
    "            #best_w = w;\n",
    "            #best_lambda = lambda_\n",
    "    \n",
    "    #print(np.shape(w))\n",
    "    gamma_step -= 1;\n",
    "    print('%d steps_gamma' % gamma_step)\n",
    "    print('gamma')\n",
    "    print(gamma_)\n",
    "        \n",
    "best_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(best_loss)\n",
    "print(best_gamma)\n",
    "np.shape(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_prediction(best_w, tx, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_w2_saved = best_w\n",
    "best_gamma2_saved = best_gamma\n",
    "best_loss2_saved = best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, gamma_, max_iter):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    train_indices = [x for j, x in enumerate(k_indices) if j != k]\n",
    "    train_indices = [idx for part in train_indices for idx in part]\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    y_train2 = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "    x_train2 = np.c_[np.ones((y_train2.shape[0], 1)), x_train]\n",
    "    w_init = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # Logistic regression\n",
    "    \n",
    "    #print(np.shape(y_train2))\n",
    "    #print(np.shape(x_train2))\n",
    "    \n",
    "    #weights_train, loss_tr = ridge_regression(y_train, train_poly, lambda_)\n",
    "    loss_tr, weights_train = logistic_regression_gradient_descent_demo(y_train2, x_train2, w_init, gamma_, max_iter, lambda_)\n",
    "    \n",
    "    return loss_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    k_fold = 2\n",
    "    max_iter = 400\n",
    "    lambdas = np.logspace(-13, 0, 15)\n",
    "    gammas = np.logspace(-13, 0, 15)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    best_loss_train = 100000\n",
    "    best_gamma = 0\n",
    "    best_lambda = 0\n",
    "    #best_w = 0\n",
    "    \n",
    "    gamma_step = len(gammas)\n",
    "    \n",
    "    # cross validation\n",
    "    for gamma_ in gammas:\n",
    "        loss_tr_tmp = []\n",
    "        lambda_step = len(lambdas)\n",
    "        \n",
    "        for lambda_ in lambdas: \n",
    "            \n",
    "            for i_test in range(k_fold): \n",
    "                loss_tr = cross_validation(y_train, x_train, k_indices, i_test, lambda_, gamma_, max_iter)\n",
    "                loss_tr_tmp.append(loss_tr)\n",
    "                print('loss  = ' + str(loss) + ', best_loss = '+ str(best_loss_train))\n",
    "            \n",
    "            lambda_step -= 1;\n",
    "            print('%d steps_lambda' % lambda_step)\n",
    "            print('lambda')\n",
    "            print(lambda_)\n",
    "            print('loss_mean')\n",
    "            print(np.sqrt(2*np.mean(loss_tr_tmp)))\n",
    "            \n",
    "            #compare_pred = compare_prediction(best_w, tx, y_train2)\n",
    "            \n",
    "            loss_train = np.sqrt(2*np.mean(loss_tr_tmp))\n",
    "            \n",
    "            if(loss_train < best_loss_train):\n",
    "                \n",
    "                best_loss_train = loss_train\n",
    "                best_gamma = gamma_\n",
    "                best_lambda = lambda_\n",
    "                #best_w = w  \n",
    "        gamma_step -= 1;\n",
    "        print('%d steps_gamma' % gamma_step)\n",
    "        print('gamma')\n",
    "        print(gamma_)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(best_loss)\n",
    "print(best_gamma)\n",
    "np.shape(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train2 = y_train.reshape(len(y_train), 1)\n",
    "tx = np.c_[np.ones((y_train2.shape[0], 1)), x_train]\n",
    "\n",
    "compare_prediction(best_w, tx, y_train2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
